{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b459f3",
   "metadata": {},
   "source": [
    "# On-Prem Iceberg Warehouse with Spark\n",
    "\n",
    "This notebook demonstrates how to query Iceberg tables stored in MinIO using Spark connected to Polaris catalog.\n",
    "All services run on-prem with no cloud connectivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4f2b3",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session\n",
    "\n",
    "The Spark session is automatically initialized with Polaris catalog configuration via PYSPARK_SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e946eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Check if spark session already exists (it should from PySpark notebook)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc48f45",
   "metadata": {},
   "source": [
    "## 2. Verify Polaris Catalog Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all catalogs available\n",
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7dee41",
   "metadata": {},
   "source": [
    "## 3. Create a Namespace (Schema) in Polaris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c2c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a namespace for your tables\n",
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS polaris.my_warehouse\")\n",
    "spark.sql(\"SHOW NAMESPACES IN polaris\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5927e472",
   "metadata": {},
   "source": [
    "## 4. Create Sample Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa54856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS polaris.my_warehouse.users (\n",
    "        id INT,\n",
    "        name STRING,\n",
    "        email STRING,\n",
    "        created_date DATE\n",
    "    )\n",
    "    USING ICEBERG\n",
    "    PARTITIONED BY (created_date)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Table created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a65d09",
   "metadata": {},
   "source": [
    "## 5. Insert Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# Insert sample data\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO polaris.my_warehouse.users VALUES\n",
    "    (1, 'Alice', 'alice@example.com', '2025-01-01'),\n",
    "    (2, 'Bob', 'bob@example.com', '2025-01-02'),\n",
    "    (3, 'Charlie', 'charlie@example.com', '2025-01-03'),\n",
    "    (4, 'Diana', 'diana@example.com', '2025-01-04')\n",
    "\"\"\")\n",
    "\n",
    "print(\"Data inserted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc5908",
   "metadata": {},
   "source": [
    "## 6. Query Using Schema.TableName (Default Catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since polaris is set as default catalog, you can query with just schema.tablename\n",
    "result = spark.sql(\"SELECT * FROM my_warehouse.users\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57273bf",
   "metadata": {},
   "source": [
    "## 7. Query with Catalog.Schema.TableName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7297a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use full 3-part name\n",
    "result = spark.sql(\"SELECT * FROM polaris.my_warehouse.users WHERE id > 2\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740df20a",
   "metadata": {},
   "source": [
    "## 8. Verify MinIO Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tables in the namespace\n",
    "spark.sql(\"SHOW TABLES IN polaris.my_warehouse\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c7b85",
   "metadata": {},
   "source": [
    "## 9. Check Table Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table schema\n",
    "spark.sql(\"DESCRIBE TABLE polaris.my_warehouse.users\").show()\n",
    "\n",
    "# View table properties\n",
    "print(\"\\n--- Table Properties ---\")\n",
    "spark.sql(\"SHOW TBLPROPERTIES polaris.my_warehouse.users\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3e1af0",
   "metadata": {},
   "source": [
    "## 10. Query with DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345481d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use DataFrame API\n",
    "df = spark.table(\"my_warehouse.users\")\n",
    "df.filter(df.id > 1).select(\"name\", \"email\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868b68b",
   "metadata": {},
   "source": [
    "## 11. Create Another Table to Demonstrate Multi-table Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create orders table\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS polaris.my_warehouse.orders (\n",
    "        order_id INT,\n",
    "        user_id INT,\n",
    "        amount DECIMAL(10,2),\n",
    "        order_date DATE\n",
    "    )\n",
    "    USING ICEBERG\n",
    "    PARTITIONED BY (order_date)\n",
    "\"\"\")\n",
    "\n",
    "# Insert sample data\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO polaris.my_warehouse.orders VALUES\n",
    "    (101, 1, 150.50, '2025-01-10'),\n",
    "    (102, 2, 200.00, '2025-01-11'),\n",
    "    (103, 1, 75.25, '2025-01-12'),\n",
    "    (104, 3, 300.00, '2025-01-13')\n",
    "\"\"\")\n",
    "\n",
    "print(\"Orders table created and populated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef27e53",
   "metadata": {},
   "source": [
    "## 12. Join Across Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f15de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join users and orders\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT u.name, u.email, o.order_id, o.amount\n",
    "    FROM my_warehouse.users u\n",
    "    JOIN my_warehouse.orders o ON u.id = o.user_id\n",
    "    ORDER BY o.order_id\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787cbe30",
   "metadata": {},
   "source": [
    "## 13. Advanced: Time Travel (Iceberg Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516edddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table history\n",
    "spark.sql(\"SELECT * FROM polaris.my_warehouse.users.history\").show()\n",
    "\n",
    "# You can query specific snapshots if needed\n",
    "# spark.sql(\"SELECT * FROM polaris.my_warehouse.users VERSION AS OF 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e370058",
   "metadata": {},
   "source": [
    "## 14. Configuration Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify your Spark configuration\n",
    "print(\"Spark SQL Catalog Config:\")\n",
    "print(f\"Default Catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n",
    "print(f\"Polaris URI: {spark.conf.get('spark.sql.catalog.polaris.uri')}\")\n",
    "print(f\"S3 Endpoint: {spark.conf.get('spark.hadoop.fs.s3a.endpoint')}\")\n",
    "print(f\"Warehouse Path: {spark.conf.get('spark.sql.catalog.polaris.warehouse')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
